```{r setup, include=FALSE}
# cluster of libraries for easier R syntax
library(tidyverse)

# library for Bayesian modeling
library(brms)
library(ggdag)
```

# Inferential Analysis: Effects of Solution-orientation

This notebook contains the inferential analysis about the effects of use case description quality.
In particular, we investigate whether solution-oriented use cases reduce or increase the time it takes to infer a solution design from a requirement.

The analysis consists of three steps according to the framework for statistical causal inference by Siebert:[^1]

1. Modeling: explicit visualization of causal assumptions
2. Identification: selection of variables from the causal model to involve in the statistical model
3. Estimation: implementation of a regression model to detect statistically significant effects.

```{r figdir}
figdir <- 'figures/inferential/effects/solutionorientation/'
```

## Modeling

We start by making our causal assumptions explicit.
The following directed, acyclic graph represents a set of variables connected by edges where we assume a causal relationship.

```{r dag}
dag <- dagify(
    # effects
    s2 ~ cplx + location + dependencies + stepsbeyond + actors.explicit,

    # interrelations
    stepsbeyond ~ location, # use cases used to specify requirements (instead of solutions) contain less steps beyond the black-box perspective
    dependencies ~ location, # use cases used to specify requirements contain more end-to-end dependencies
    actors.explicit ~ location, # the more actors in a (proper requirements) use case, the more actors are also explicity mentioned


    # causes
    textorder ~ owner + author + cplx,
    location ~ owner + author + cplx,
    dependencies ~ owner + author + cplx,
    stepsbeyond ~ owner + author + cplx,
    actors.explicit ~ owner + author + cplx,

    exposure = "stepsbeyond",
    outcome = "s2",
    labels = c(owner = "requirements owner", author = "requirements author", cplx = "complexity",
                location = "use case location", dependencies = "end-to-end dependencies", stepsbeyond = "white-box steps", actors.explicit = "number of explicit actors", 
                s2 = "design time"),

    coords = list(
        x = c(owner = 0, author = 0, cplx = 0,
                location = 1, dependencies = 1, stepsbeyond = 2, actors.explicit = 1,
                s2 = 3),
        y = c(owner = 0, author = 1, cplx = 2,
                location = 1, dependencies = 2, stepsbeyond = 1.5, actors.explicit = 0,
                s2 = 1.5))
)

ggdag_status(dag, use_labels="label", text = FALSE) +
  guides(fill = "none", color = "none") + 
  theme_dag()

ggsave(paste0(figdir, 'dag.pdf'), width=6, height=3)
```

## Identification

To deconfound the effect of the exposure on the outcome variable, we need to identify the adjustment set, i.e., the set of variables to control.

```{r identification}
ggdag_adjustment_set(dag, exposure = "stepsbeyond", outcome = "s2") +
  theme(axis.text.x = element_blank(), axis.ticks.x = element_blank(), axis.title.x = element_blank(), 
        axis.text.y = element_blank(), axis.ticks.y = element_blank(), axis.title.y = element_blank())
```

## Estimation

In the third step, we estimate the average causal effect (ACE) of the exposure on the outcome.

```{r data-loading}
d <- read.csv('data/output/uca-aggregated.csv')
d$cplx <- factor(d$cplx, levels=c("Low Complexity", "Medium Complexity", "High Complexity"), ordered=TRUE)
d$location <- factor(d$location, levels=c("Business Background", "Business Requirements", "Solution Proposal", "Default"), ordered=FALSE)

d  <- d[complete.cases(d$cplx), ]
d <- d  %>% filter(nuc > 0 & form == "description" & procedural > 0)
```

### Formula and Distribution

The following regression formula represents our statistical model derived from the causal model.

```{r formula}
f <- bf(s2 ~ cplx + steps.beyond + location + actors_explicit + dependencies)
```

We already determined in the [adoption](adoption.Rmd) analysis that the `zero_inflated_negbinomial` distribution models the response variable best.

### Priors

We need to assign a prior distribution to the coefficient of each predictor involved in the statistical model.

```{r eligible-priors}
get_prior(f, family = zero_inflated_negbinomial, data = d)
```

We select uninformative priors for each coefficient.

```{r set-priors}
priors <- c(
    prior(normal(0, 1), class = Intercept),
    prior(normal(0, 0.5), class = b),
    prior(gamma(2, 3), class=shape),
    prior(beta(1, 3), class=zi)
)
```

To confirm the eligibility of the priors, we run a prior predictive check.
To this end, we sample from the model without updating the coefficients based on the data.

```{r model-prior}
m.prior <-
  brm(data = d, family = zero_inflated_negbinomial, f, prior = priors,
    iter = 4000, warmup = 1000, chains = 4, cores = 4,
    seed = 4, sample_prior="only",
    file = "./src/analysis/fits/effect.solution.prior"
  )
```

Then, we draw 100 random samples from the prior model to assess, whether the priors are proper.

```{r prior-predictive-check}
brms::pp_check(m.prior, ndraws = 100)
```

Note that we would typically aim to see that the samples $y_{rep}$ encompass the actually observed data $y$. 
This may not be the case for the mocked data.

### Model Training

With the priors confirmed, we can train the model, i.e., update the coefficients based on the observed data.

```{r model}
m <-
  brm(data = d, family = zero_inflated_negbinomial, f, prior = priors,
    iter = 4000, warmup = 1000, chains = 4, cores = 4,
    seed = 4, 
    file = "./src/analysis/fits/effect.solution"
  )
```

Similarly, we can draw 100 random samples from the trained model to confirm that the model has updated its priors accordingly.

```{r posterior-predictive-check}
brms::pp_check(m, ndraws = 100)
```

Since the predicted values $y_{rep}$ have grown closer to the actually observed values $y$, we can confirm that the model has properly trained and improved its predictive power.

### Evaluation

Finally, we can evaluate the model and derive conclusions from it.

#### Path Coefficients

To evaluate the model, we can inspect the posterior distribution of the path coefficients.

```{r coefficients}
summary(m)
```

These can also be visualized in an `mcmc_plot`.
Factors where the confidence does not overlap with 0 can be considered significant.

```{r mcmc-plot}
mcmc_plot(m)
```

#### Marginal Plots

Additionally, we can plot the marginal effects of specific predictors on the response variable of interest.
These marginal effects represent the isolated effect of the predictor on the response variable while holding all other predictors constant at representative values.

```{r marginal-cplx}
c_eff <- conditional_effects(m, effects="cplx")
plot(c_eff, plot = FALSE)[[1]] +
  geom_hline(yintercept = 0, color = "grey", linetype = "dashed") +
  scale_x_discrete(limits=rev) +
  coord_flip() +
  labs(y = "Design Time") +
  theme(axis.title.y = element_blank()) 

#ggsave(paste0(figdir, 'marginal-cplx.pdf'), width=4, height=1)
```

```{r marginal-location}
c_eff <- conditional_effects(m, effects="location")
plot(c_eff, plot = FALSE)[[1]] +
  geom_hline(yintercept = 0, color = "grey", linetype = "dashed") +
  scale_x_discrete(limits=rev) +
  coord_flip() +
  labs(y = "Design Time") +
  theme(axis.title.y = element_blank()) 

#ggsave(paste0(figdir, 'marginal-location.pdf'), width=4, height=1.3)
```

```{r marginal-dependencies}
conditional_effects(m, effects="dependencies")
```

```{r marginal-steps-beyond}
c_eff <- conditional_effects(m, effects="steps.beyond")
plot(c_eff, plot = FALSE)[[1]] +
  geom_hline(yintercept = 0, color = "grey", linetype = "dashed") +
  labs(y = "Design Time", x="Number of white-box steps")

#ggsave(paste0(figdir, 'marginal-stepsbeyond.pdf'), width=4, height=2)
```

```{r marginal-explicit-actors}
c_eff <- conditional_effects(m, effects="actors_explicit")
plot(c_eff, plot = FALSE)[[1]] +
  geom_hline(yintercept = 0, color = "grey", linetype = "dashed") +
  labs(y = "Design Time", x="Number of explicit actors") + 
  scale_x_continuous(breaks=c(0, 1, 2))
  
#ggsave(paste0(figdir, 'marginal-explicitactors.pdf'), width=4, height=2)
```

[^1]: Siebert, J. (2023). Applications of statistical causal inference in software engineering. Information and Software Technology, 159, 107198.